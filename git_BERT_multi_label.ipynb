{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wsIJtutj5X-"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall transformers==4.52.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nKaJ6Yqgpr6"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers datasets scikit-learn pandas accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okMVEDjij8qy"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h0eMW_mlNy7"
      },
      "outputs": [],
      "source": [
        "berargs = TrainingArguments(output_dir=\"./results\")\n",
        "# print(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX4mtuWAhs5A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "1r9aSVlcwqpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"df_combined.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5gJ6ghJMwXuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL33_ysThs5A"
      },
      "outputs": [],
      "source": [
        "y = df.drop(columns=[\"questionText\", \"category_type\", \"answer\"])\n",
        "X=df[[\"questionText\", \"answer\"]]\n",
        "#3. Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Tokenization\n",
        "# bert-base-german-cased\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_texts(text_a, text_b, labels):\n",
        "    encodings = tokenizer(\n",
        "        text=text_a.tolist(),\n",
        "        text_pair=text_b.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encodings['labels'] = torch.tensor(labels, dtype=torch.float)\n",
        "    return encodings\n",
        "\n",
        "train_encodings = tokenize_texts(\n",
        "    X_train[\"questionText\"],\n",
        "    X_train[\"answer\"],\n",
        "    y_train.values\n",
        ")\n",
        "\n",
        "test_encodings = tokenize_texts(\n",
        "    X_test[\"questionText\"],\n",
        "    X_test[\"answer\"],\n",
        "    y_test.values\n",
        ")\n"
      ],
      "metadata": {
        "id": "En6OwohhPN3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurveyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "train_dataset = SurveyDataset(train_encodings)\n",
        "test_dataset = SurveyDataset(test_encodings)"
      ],
      "metadata": {
        "id": "kYRtf9CMRCFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C10xeeOmhs5B"
      },
      "outputs": [],
      "source": [
        "#6. Model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=y.shape[1],\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2CFVVP9hRJf"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Threshold grid\n",
        "thresholds_to_try = np.arange(0.1, 0.9, 0.1)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, true_labels = pred\n",
        "\n",
        "    # Ensure numpy arrays (avoid creating a torch.Tensor each time)\n",
        "    if not isinstance(logits, np.ndarray):\n",
        "        try:\n",
        "            logits = np.asarray(logits)\n",
        "        except Exception:\n",
        "            logits = np.array(logits)  # fallback\n",
        "\n",
        "    if not isinstance(true_labels, np.ndarray):\n",
        "        true_labels = np.asarray(true_labels)\n",
        "\n",
        "    # Sigmoid to get probabilities (vectorized)\n",
        "    probs = 1 / (1 + np.exp(-logits))  # equivalent to torch.sigmoid\n",
        "\n",
        "    num_labels = true_labels.shape[1]\n",
        "    best_thresholds = np.zeros(num_labels, dtype=float)\n",
        "    final_preds = np.zeros_like(true_labels)\n",
        "\n",
        "    # Per-label threshold search\n",
        "    for i in range(num_labels):\n",
        "        # Broadcast comparisons and metric calculation\n",
        "        best_f1 = -1.0\n",
        "        best_thresh = 0.5\n",
        "        for thresh in thresholds_to_try:\n",
        "            preds_i = (probs[:, i] > thresh).astype(int)\n",
        "            f1 = f1_score(true_labels[:, i], preds_i, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = thresh\n",
        "        best_thresholds[i] = best_thresh\n",
        "        final_preds[:, i] = (probs[:, i] > best_thresh).astype(int)\n",
        "\n",
        "    # Compute aggregated metrics\n",
        "    f1_micro = f1_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "    f1_macro = f1_score(true_labels, final_preds, average=\"macro\", zero_division=0)\n",
        "    precision_micro = precision_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "    recall_micro = recall_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"f1_micro\": f1_micro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"precision_micro\": precision_micro,\n",
        "        \"recall_micro\": recall_micro,\n",
        "        # Convert to plain Python list so Trainer can serialize without error\n",
        "        \"best_thresholds\": best_thresholds.tolist(),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WYl_2lpq3sNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4QGRNI7hs5B"
      },
      "outputs": [],
      "source": [
        "#7. Training\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# === Prepare directory ===\n",
        "local_dir = \"./my_trained_model\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# === 1. Save model, tokenizer, training args ===\n",
        "trainer.save_model(local_dir)\n",
        "tokenizer.save_pretrained(local_dir)\n",
        "\n",
        "with open(os.path.join(local_dir, \"training_args.json\"), \"w\") as f:\n",
        "    f.write(trainer.args.to_json_string())\n",
        "\n",
        "# === 2. Save label names from y columns ===\n",
        "label_names = y.columns.tolist()  # <--- Make sure y is a DataFrame\n",
        "with open(os.path.join(local_dir, \"label_names.json\"), \"w\") as f:\n",
        "    json.dump(label_names, f, indent=2)\n",
        "\n",
        "threshold_last = None\n",
        "# Traverse log_history in reverse to find the latest entry with best_thresholds\n",
        "for log in reversed(trainer.state.log_history):\n",
        "    if \"eval_best_thresholds\" in log:\n",
        "        threshold_last = log[\"eval_best_thresholds\"]\n",
        "        break\n",
        "\n",
        "if threshold_last is not None:\n",
        "    with open(os.path.join(local_dir, \"best_thresholds_last_epoch.json\"), \"w\") as f:\n",
        "        json.dump(threshold_last, f, indent=2)\n",
        "    print(\"✅ Saved best_thresholds from the last epoch.\")\n",
        "else:\n",
        "    print(\"⚠️ No thresholds found in any epoch.\")\n",
        "\n",
        "with open(os.path.join(local_dir, \"log_history.json\"), \"w\") as f:\n",
        "    json.dump(trainer.state.log_history, f, indent=2)\n",
        "\n",
        "# === 4. Zip and download ===\n",
        "shutil.make_archive(\"my_trained_model\", \"zip\", local_dir)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"my_trained_model.zip\")\n"
      ],
      "metadata": {
        "id": "xFr6S-pBv4rA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}