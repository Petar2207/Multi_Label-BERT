{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wsIJtutj5X-"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall transformers==4.52.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nKaJ6Yqgpr6"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers datasets scikit-learn pandas accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okMVEDjij8qy"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h0eMW_mlNy7"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(output_dir=\"./results\")\n",
        "# print(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX4mtuWAhs5A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r9aSVlcwqpv"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "id": "DGcRnYhOc_af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel(\"1.xlsx\")"
      ],
      "metadata": {
        "id": "xSGAF6-sdVNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0y6zAlJgOnP"
      },
      "outputs": [],
      "source": [
        "# Merge question + text\n",
        "df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
        "df['labels'] = df['categoriesString'].apply(lambda x: [c.strip() for c in x.split(',')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the 'text' column is NaN\n",
        "df = df.dropna(subset=[\"questionText\", \"answer\", \"labels\"])\n",
        "\n",
        "# Optionally, reset the index after dropping rows (if needed)\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Pf3L0O5ggOnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FwUGLhLFBCOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[[\"questionText\", \"answer\",\"labels\"]]"
      ],
      "metadata": {
        "id": "bluu53IjQCSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the encoder\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Fit and transform the categories to boolean values\n",
        "encoded = mlb.fit_transform(df['labels'])\n",
        "\n",
        "# Create a new DataFrame with the encoded categories\n",
        "encoded_df = pd.DataFrame(encoded, columns=mlb.classes_, index=df.index)\n",
        "\n",
        "# Concatenate with the original DataFrame (excluding 'categories' column)\n",
        "df_encoded = pd.concat([df.drop('labels', axis=1), encoded_df], axis=1)"
      ],
      "metadata": {
        "id": "nHRr_xq_gOnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_encoded"
      ],
      "metadata": {
        "id": "9wG6EggFgOnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.drop(columns=[\"questionText\", \"answer\"])"
      ],
      "metadata": {
        "id": "Cj4DkYAogOnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minority_columns = y.sum()"
      ],
      "metadata": {
        "id": "mHsUAAt43CtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10 = minority_columns.nlargest(1).index.tolist()"
      ],
      "metadata": {
        "id": "Cedk7Xu9060H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10"
      ],
      "metadata": {
        "id": "laHr1zL_1R1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minority_columns[[\"Entlohnung\"]]"
      ],
      "metadata": {
        "id": "xIdSUmf621ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 posto\n",
        "below_94 = minority_columns[minority_columns < 94].index.tolist()\n",
        "\n",
        "print(below_94)\n"
      ],
      "metadata": {
        "id": "I2rw_-RD-fxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "\n",
        "def collect_and_pad_positive_rows(df, columns, target_n=94):\n",
        "    \"\"\"\n",
        "    Collects all rows where each column == 1, and upscales to target_n rows per column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The full dataset\n",
        "        columns (list): List of binary column names to check (e.g., [\"x\", \"y\"])\n",
        "        target_n (int): Minimum number of rows to keep per class\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with keys = column names and values = padded DataFrames\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for col in columns:\n",
        "        subset = df[df[col] == 1].copy()\n",
        "        n_rows = len(subset)\n",
        "\n",
        "        if n_rows < target_n:\n",
        "            # Upsample with replacement\n",
        "            subset = resample(\n",
        "                subset,\n",
        "                replace=True,\n",
        "                n_samples=target_n,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "        results[col] = subset\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "OpmdfowIgOnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_check = below_94\n",
        "my_result = collect_and_pad_positive_rows(df, columns_to_check, target_n=94)\n"
      ],
      "metadata": {
        "id": "OF973nJSgOnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col, df_upsampled in my_result.items():\n",
        "    count = df_upsampled[col].sum()\n",
        "    total = len(df_upsampled)\n",
        "    print(f\"{col}: total rows = {total}, positive labels = {count}\")\n"
      ],
      "metadata": {
        "id": "_kta2exBgOnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all upsampled DataFrames from your dict\n",
        "padded_df = pd.concat(my_result.values()).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "tH5At0c9gOnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove original minority rows\n",
        "df_filtered = df.copy()\n",
        "for col in columns_to_check:\n",
        "    df_filtered = df_filtered[df_filtered[col] != 1]\n",
        "\n",
        "# 2. Combine the filtered df with the upsampled data\n",
        "df = pd.concat([df_filtered, padded_df]).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "urWJRALCgOnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eKDLTY2ygN7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jkSpEBngN-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7i5jNIEBgOCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Gd3r2PggOFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBIts0kdgOIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "no3ACFlQgOMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL33_ysThs5A"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features and labels\n",
        "y = df.drop(columns=[\"questionText\", \"answer\"])\n",
        "X = df[[\"questionText\", \"answer\"]]\n",
        "\n",
        "# First split: separate out the test set (10%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: from remaining 90%, take 10% for validation (so 9% total)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En6OwohhPN3d"
      },
      "outputs": [],
      "source": [
        "# 4. Tokenization\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_texts(text_a, text_b, labels):\n",
        "    encodings = tokenizer(\n",
        "        text=text_a.tolist(),\n",
        "        text_pair=text_b.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encodings['labels'] = torch.tensor(labels, dtype=torch.float)\n",
        "    return encodings\n",
        "\n",
        "# Train encodings\n",
        "train_encodings = tokenize_texts(\n",
        "    X_train[\"questionText\"],\n",
        "    X_train[\"answer\"],\n",
        "    y_train.values\n",
        ")\n",
        "\n",
        "# Validation encodings (NEW)\n",
        "val_encodings = tokenize_texts(\n",
        "    X_val[\"questionText\"],\n",
        "    X_val[\"answer\"],\n",
        "    y_val.values\n",
        ")\n",
        "\n",
        "# Test encodings\n",
        "test_encodings = tokenize_texts(\n",
        "    X_test[\"questionText\"],\n",
        "    X_test[\"answer\"],\n",
        "    y_test.values\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYRtf9CMRCFb"
      },
      "outputs": [],
      "source": [
        "class SurveyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "train_dataset = SurveyDataset(train_encodings)\n",
        "val_dataset = SurveyDataset(val_encodings)\n",
        "test_dataset = SurveyDataset(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C10xeeOmhs5B"
      },
      "outputs": [],
      "source": [
        "#6. Model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=y.shape[1],\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”’ Freeze lower layers and unfreeze top 4 + classifier\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in [\n",
        "        \"encoder.layer.8\",\n",
        "        \"encoder.layer.9\",\n",
        "        \"encoder.layer.10\",\n",
        "        \"encoder.layer.11\"\n",
        "    ]):\n",
        "        param.requires_grad = True\n",
        "\n",
        "    if \"classifier\" in name:\n",
        "        param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "jOEbPStQWpKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2CFVVP9hRJf"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "#Custom trainer u kojme se loss funkcija kustomizita ia ubacuje wieght\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYl_2lpq3sNC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Threshold grid\n",
        "thresholds_to_try = np.arange(0.1, 0.9, 0.01)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, true_labels = pred\n",
        "\n",
        "    # Ensure numpy arrays (avoid creating a torch.Tensor each time)\n",
        "    if not isinstance(logits, np.ndarray):\n",
        "        try:\n",
        "            logits = np.asarray(logits)\n",
        "        except Exception:\n",
        "            logits = np.array(logits)  # fallback\n",
        "\n",
        "    if not isinstance(true_labels, np.ndarray):\n",
        "        true_labels = np.asarray(true_labels)\n",
        "\n",
        "    # Sigmoid to get probabilities (vectorized)\n",
        "    probs = 1 / (1 + np.exp(-logits))  # equivalent to torch.sigmoid\n",
        "\n",
        "    num_labels = true_labels.shape[1]\n",
        "    best_thresholds = np.zeros(num_labels, dtype=float)\n",
        "    final_preds = np.zeros_like(true_labels)\n",
        "\n",
        "    # Per-label threshold search\n",
        "    for i in range(num_labels):\n",
        "        # Broadcast comparisons and metric calculation\n",
        "        best_f1 = -1.0\n",
        "        best_thresh = 0.5\n",
        "        for thresh in thresholds_to_try:\n",
        "            preds_i = (probs[:, i] > thresh).astype(int)\n",
        "            f1 = f1_score(true_labels[:, i], preds_i, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = thresh\n",
        "        best_thresholds[i] = best_thresh\n",
        "        final_preds[:, i] = (probs[:, i] > best_thresh).astype(int)\n",
        "\n",
        "    # Compute aggregated metrics\n",
        "    f1_micro = f1_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "    f1_macro = f1_score(true_labels, final_preds, average=\"macro\", zero_division=0)\n",
        "    precision_micro = precision_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "    recall_micro = recall_score(true_labels, final_preds, average=\"micro\", zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"f1_micro\": f1_micro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"precision_micro\": precision_micro,\n",
        "        \"recall_micro\": recall_micro,\n",
        "        # Convert to plain Python list so Trainer can serialize without error\n",
        "        \"best_thresholds\": best_thresholds.tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Training\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_model\",\n",
        "\n",
        "    # âœ… Corrected parameter name\n",
        "    eval_strategy=\"epoch\",    # evaluates at the end of each epoch\n",
        "    save_strategy=\"epoch\",          # saves model checkpoint each epoch\n",
        "\n",
        "    # âœ… Learning parameters (still flexible to tune later)\n",
        "    learning_rate=2e-5,             # can be tuned later or scheduled dynamically\n",
        "    lr_scheduler_type=\"linear\",     # gradually decreases LR over time\n",
        "    warmup_ratio=0.1,               # 10% of steps for learning rate warmup\n",
        "\n",
        "    # âœ… Batch & epoch settings\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=11,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # âœ… Logging & saving\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # âœ… Best model & metric\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_micro\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,        # prevents uploading anything to HF Hub\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,           # âœ… uses validation set (not test)\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        "    # âœ… Early stopping to avoid overfitting\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n"
      ],
      "metadata": {
        "id": "-XnHvhTJLySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "1m9UcmD2Btz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "ZAZVkTmBGqxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# === Prepare directory ===\n",
        "local_dir = \"./my_trained_model\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# === 1. Save the best model + tokenizer + training args ===\n",
        "# (If training finished normally, trainer.model is already the best model)\n",
        "trainer.save_model(local_dir)\n",
        "tokenizer.save_pretrained(local_dir)\n",
        "\n",
        "with open(os.path.join(local_dir, \"training_args.json\"), \"w\") as f:\n",
        "    f.write(trainer.args.to_json_string())\n",
        "\n",
        "# === 2. Save label names ===\n",
        "label_names = y.columns.tolist()  # y should be your label DataFrame\n",
        "with open(os.path.join(local_dir, \"label_names.json\"), \"w\") as f:\n",
        "    json.dump(label_names, f, indent=2)\n",
        "\n",
        "# === 3. Find thresholds for the best model ===\n",
        "threshold_best = None\n",
        "metric_key = \"eval_\" + trainer.args.metric_for_best_model\n",
        "best_metric = trainer.state.best_metric\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if metric_key in log and abs(log[metric_key] - best_metric) < 1e-12:\n",
        "        if \"eval_best_thresholds\" in log:\n",
        "            threshold_best = log[\"eval_best_thresholds\"]\n",
        "        break\n",
        "\n",
        "if threshold_best is not None:\n",
        "    with open(os.path.join(local_dir, \"best_thresholds.json\"), \"w\") as f:\n",
        "        json.dump(threshold_best, f, indent=2)\n",
        "    print(\"âœ… Saved thresholds matching the best model.\")\n",
        "else:\n",
        "    print(\"âš ï¸ No thresholds found for the best model.\")\n",
        "\n",
        "# === 4. Save log history (optional but useful) ===\n",
        "with open(os.path.join(local_dir, \"log_history.json\"), \"w\") as f:\n",
        "    json.dump(trainer.state.log_history, f, indent=2)\n",
        "\n",
        "# === 5. Zip and download (for Colab) ===\n",
        "zip_path = shutil.make_archive(\"my_trained_model\", \"zip\", local_dir)\n",
        "print(f\"ðŸ“¦ Zipped to: {zip_path}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_path)\n",
        "except Exception:\n",
        "    print(\"âœ… Saved locally (not in Colab).\")\n"
      ],
      "metadata": {
        "id": "U7INGI-_cOrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load log history from file\n",
        "with open(\"./my_trained_model/log_history.json\", \"r\") as f:\n",
        "    log_history = json.load(f)\n",
        "\n",
        "# Print evaluation entries (with metrics like F1, precision, thresholds, etc.)\n",
        "print(\"ðŸ“Š Evaluation Metrics per Epoch:\\n\")\n",
        "for entry in log_history:\n",
        "    if \"eval_loss\" in entry:  # Only show evaluation steps\n",
        "        print(f\"Epoch {entry.get('epoch', '?')} (Step {entry.get('step', '?')}):\")\n",
        "        print(f\"  - Eval Loss         : {entry.get('eval_loss'):.4f}\")\n",
        "        print(f\"  - F1 Micro          : {entry.get('eval_f1_micro'):.4f}\")\n",
        "        print(f\"  - F1 Macro          : {entry.get('eval_f1_macro'):.4f}\")\n",
        "        print(f\"  - Precision (Micro) : {entry.get('eval_precision_micro'):.4f}\")\n",
        "        print(f\"  - Recall (Micro)    : {entry.get('eval_recall_micro'):.4f}\")\n",
        "        print(f\"  - Thresholds (len)  : {len(entry.get('eval_best_thresholds', []))} labels\")\n",
        "        print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "kV0iixSPeC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"./my_trained_model/best_thresholds.json\", \"r\") as f:\n",
        "    thresholds = json.load(f)\n",
        "\n",
        "print(\"ðŸ“Š Best thresholds from last epoch:\")\n",
        "for i, t in enumerate(thresholds):\n",
        "    print(f\"Label {i}: Threshold = {t}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dj8mdEcreKeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}