{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wsIJtutj5X-"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall transformers==4.52.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nKaJ6Yqgpr6"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers datasets scikit-learn pandas accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okMVEDjij8qy"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h0eMW_mlNy7"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(output_dir=\"./results\")\n",
        "# print(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX4mtuWAhs5A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r9aSVlcwqpv"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGcRnYhOc_af"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSGAF6-sdVNP"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"faza_1_training_dataset_sa_comma.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uzzSTCTlTxR"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSenLDDNlWOe"
      },
      "outputs": [],
      "source": [
        "df=df[[\"questionText\", \"questionAnswer\",\"category\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0y6zAlJgOnP"
      },
      "outputs": [],
      "source": [
        "df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
        "df['labels'] = df['category'].apply(lambda x: [c.strip() for c in x.split(',')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf3L0O5ggOnP"
      },
      "outputs": [],
      "source": [
        "# Drop rows where the 'text' column is NaN\n",
        "df = df.dropna(subset=[\"questionText\", \"questionAnswer\", \"labels\"])\n",
        "\n",
        "# Optionally, reset the index after dropping rows (if needed)\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lTBcTkGOnc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "df[\"questionAnswer\"] = df[\"questionAnswer\"].replace(\"#NAME?\", np.nan)\n",
        "\n",
        "df = df.dropna(subset=[\"questionAnswer\"]).copy()\n",
        "df = df[df[\"questionAnswer\"].astype(str).str.strip().ne(\"\")].copy()\n",
        "\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bluu53IjQCSu"
      },
      "outputs": [],
      "source": [
        "df=df[[\"questionText\", \"questionAnswer\",\"labels\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UMAU6pd-jz_"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHRr_xq_gOnP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# (optional but recommended) normalize answer so Nein/nein match\n",
        "df[\"questionAnswer_norm\"] = (\n",
        "    df[\"questionAnswer\"]\n",
        "      .astype(str)\n",
        "      .str.strip()\n",
        "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "      .str.lower()\n",
        ")\n",
        "\n",
        "# 1) Create group id for each unique (questionText, normalized answer)\n",
        "df[\"pair_id\"] = pd.util.hash_pandas_object(\n",
        "    df[[\"questionText\", \"questionAnswer_norm\"]],\n",
        "    index=False\n",
        ").astype(\"int64\")\n",
        "\n",
        "group_col = \"pair_id\"\n",
        "\n",
        "# 2) Build ONE label-list per group (union of labels inside the group)\n",
        "grouped = (\n",
        "    df.groupby(group_col)[\"labels\"]\n",
        "      .apply(lambda s: sorted(set(l for labs in s for l in labs)))\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "groups = grouped[group_col].values\n",
        "group_labels_list = grouped[\"labels\"]\n",
        "\n",
        "# 3) Multi-hot for stratifying groups\n",
        "mlb_groups = MultiLabelBinarizer()\n",
        "Y_groups = mlb_groups.fit_transform(group_labels_list)\n",
        "\n",
        "# --- Split groups: trainval vs test (10%) ---\n",
        "msss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=42)\n",
        "trainval_g_idx, test_g_idx = next(msss1.split(np.zeros(len(groups)), Y_groups))\n",
        "\n",
        "trainval_groups = set(groups[trainval_g_idx])\n",
        "test_groups     = set(groups[test_g_idx])\n",
        "\n",
        "df_trainval = df[df[group_col].isin(trainval_groups)].copy()\n",
        "df_test     = df[df[group_col].isin(test_groups)].copy()\n",
        "\n",
        "# --- Split trainval groups: train vs val (val = 10% total => 0.111111 of trainval) ---\n",
        "grouped_trainval = grouped[grouped[group_col].isin(trainval_groups)].reset_index(drop=True)\n",
        "groups_tv = grouped_trainval[group_col].values\n",
        "labels_tv = grouped_trainval[\"labels\"]\n",
        "\n",
        "mlb_tv = MultiLabelBinarizer()\n",
        "Y_tv = mlb_tv.fit_transform(labels_tv)\n",
        "\n",
        "msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.111111, random_state=42)\n",
        "train_g_idx, val_g_idx = next(msss2.split(np.zeros(len(groups_tv)), Y_tv))\n",
        "\n",
        "train_groups = set(groups_tv[train_g_idx])\n",
        "val_groups   = set(groups_tv[val_g_idx])\n",
        "\n",
        "df_train = df_trainval[df_trainval[group_col].isin(train_groups)].copy()\n",
        "df_val   = df_trainval[df_trainval[group_col].isin(val_groups)].copy()\n",
        "\n",
        "# --- 4) Build X ---\n",
        "X_train = df_train[[\"questionText\", \"questionAnswer\"]]\n",
        "X_val   = df_val[[\"questionText\", \"questionAnswer\"]]\n",
        "X_test  = df_test[[\"questionText\", \"questionAnswer\"]]\n",
        "\n",
        "# --- 5) Fit MultiLabelBinarizer on TRAIN ONLY (clean) ---\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(df_train[\"labels\"])\n",
        "y_val   = mlb.transform(df_val[\"labels\"])\n",
        "y_test  = mlb.transform(df_test[\"labels\"])\n",
        "\n",
        "# Optional: y as DataFrames\n",
        "y_train = pd.DataFrame(y_train, columns=mlb.classes_, index=df_train.index)\n",
        "y_val   = pd.DataFrame(y_val,   columns=mlb.classes_, index=df_val.index)\n",
        "y_test  = pd.DataFrame(y_test,  columns=mlb.classes_, index=df_test.index)\n",
        "\n",
        "print(len(df_train), len(df_val), len(df_test))\n",
        "print(\"Num labels:\", len(mlb.classes_))\n",
        "print(\"Labels missing from train:\", set().union(*df[\"labels\"]) - set().union(*df_train[\"labels\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En6OwohhPN3d"
      },
      "outputs": [],
      "source": [
        "# 4. Tokenization (Option B: German model)\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# checkpoint = \"deepset/gbert-base\"   # German BERT (recommended for German data)\n",
        "checkpoint = \"deepset/gbert-large\"   # German BERT (recommended for German data)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_texts(text_a, text_b, labels):\n",
        "    encodings = tokenizer(\n",
        "        text=text_a.tolist(),\n",
        "        text_pair=text_b.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encodings[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
        "    return encodings\n",
        "\n",
        "# Train encodings\n",
        "train_encodings = tokenize_texts(\n",
        "    X_train[\"questionText\"],\n",
        "    X_train[\"questionAnswer\"],\n",
        "    y_train.values\n",
        ")\n",
        "\n",
        "# Validation encodings\n",
        "val_encodings = tokenize_texts(\n",
        "    X_val[\"questionText\"],\n",
        "    X_val[\"questionAnswer\"],\n",
        "    y_val.values\n",
        ")\n",
        "\n",
        "# Test encodings\n",
        "test_encodings = tokenize_texts(\n",
        "    X_test[\"questionText\"],\n",
        "    X_test[\"questionAnswer\"],\n",
        "    y_test.values\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYRtf9CMRCFb"
      },
      "outputs": [],
      "source": [
        "class SurveyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "train_dataset = SurveyDataset(train_encodings)\n",
        "val_dataset = SurveyDataset(val_encodings)\n",
        "test_dataset = SurveyDataset(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C10xeeOmhs5B"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"deepset/gbert-base\",\n",
        "#     num_labels=y_train.shape[1],\n",
        "#     problem_type=\"multi_label_classification\"\n",
        "# )\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"deepset/gbert-large\",\n",
        "    num_labels=y_train.shape[1],\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOEbPStQWpKv"
      },
      "outputs": [],
      "source": [
        "# Freeze everything, then unfreeze top 4 encoder layers + pooler + classifier\n",
        "# Works with BertForSequenceClassification / AutoModelForSequenceClassification (BERT-style)\n",
        "\n",
        "def freeze_all_then_unfreeze_top_k_bert(model, k=12):\n",
        "    # 1) freeze all params\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 2) unfreeze top-k encoder layers (BERT-base has 12 layers: 0..11)\n",
        "    top_layers = list(range(12 - k, 12))  # e.g. k=4 -> [8,9,10,11]\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        # encoder layers\n",
        "        if any(f\"encoder.layer.{i}.\" in name for i in top_layers):\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # optional: pooler (small, often helps)\n",
        "        if \"pooler\" in name:\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # classifier head\n",
        "        if \"classifier\" in name:\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # 3) print trainable params\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Unfroze top {k} layers + pooler + classifier\")\n",
        "    print(f\"Trainable params: {trainable:,} / {total:,} ({trainable/total:.2%})\")\n",
        "\n",
        "# --- use it ---\n",
        "freeze_all_then_unfreeze_top_k_bert(model, k=12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub6rACk_WgKg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# y_train: DataFrame or numpy array with shape (n_samples, n_labels), values {0,1}\n",
        "Y = y_train.values if hasattr(y_train, \"values\") else y_train\n",
        "\n",
        "pos = Y.sum(axis=0)                 # positives per label\n",
        "neg = Y.shape[0] - pos              # negatives per label\n",
        "\n",
        "# avoid division by zero (shouldn't happen if every label exists in train)\n",
        "pos_weight = neg / np.clip(pos, 1, None)\n",
        "\n",
        "pos_weight_t = torch.tensor(pos_weight, dtype=torch.float32)\n",
        "print(\"pos_weight shape:\", pos_weight_t.shape)  # (num_labels,)\n",
        "print(\"pos_weight min/median/max:\", pos_weight_t.min().item(), np.median(pos_weight), pos_weight_t.max().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2CFVVP9hRJf"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class WeightedMultilabelTrainer(Trainer):\n",
        "    def __init__(self, *args, pos_weight=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.pos_weight = pos_weight  # torch tensor shape (num_labels,)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\").float()  # ensure float32\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Make sure pos_weight is on same device as logits\n",
        "        if self.pos_weight is not None:\n",
        "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
        "        else:\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYl_2lpq3sNC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def compute_metrics(eval_pred, threshold=0.5, force_top1=False):\n",
        "    logits, y_true = eval_pred  # shapes: (N, C)\n",
        "    probs = sigmoid(logits)\n",
        "\n",
        "    # threshold-based multi-label prediction\n",
        "    y_pred = (probs >= threshold).astype(int)\n",
        "\n",
        "    # Optional: guarantee at least one label per sample (useful for your goal)\n",
        "    if force_top1:\n",
        "        empty = (y_pred.sum(axis=1) == 0)\n",
        "        if np.any(empty):\n",
        "            top1 = probs[empty].argmax(axis=1)\n",
        "            y_pred[empty] = 0\n",
        "            y_pred[empty, top1] = 1\n",
        "\n",
        "    # --- Coverage / emptiness metrics ---\n",
        "    pred_counts = y_pred.sum(axis=1)                    # how many labels predicted per sample\n",
        "    coverage = np.mean(pred_counts > 0)                # fraction with at least 1 label\n",
        "    avg_labels = float(np.mean(pred_counts))           # label cardinality (avg predicted labels per sample)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"precision_micro\": precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "        \"recall_micro\": recall_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "\n",
        "        # new: usefulness/behavior metrics\n",
        "        \"coverage_at_least_one\": float(coverage),\n",
        "        \"avg_pred_labels\": avg_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XnHvhTJLySs"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_model\",\n",
        "\n",
        "    eval_strategy=\"epoch\",         # your transformers version uses this\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    per_device_train_batch_size=16,   # H100 can handle this usually\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=30,               # early stopping will stop earlier if needed\n",
        "\n",
        "    bf16=True,                        # âœ… best on H100\n",
        "    dataloader_pin_memory=True,       # fine on GPU\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_micro\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "\n",
        "trainer = WeightedMultilabelTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,  # fixed threshold metrics during training\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    pos_weight=pos_weight_t,          # âœ… your computed pos_weight tensor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m9UcmD2Btz5"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
        "print(\"Best metric:\", trainer.state.best_metric)\n"
      ],
      "metadata": {
        "id": "TrSnIvEbnv7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "print(\"Last checkpoint:\", get_last_checkpoint(\"./output_model\"))\n"
      ],
      "metadata": {
        "id": "mddQIYM5qR2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def find_best_thresholds(logits, y_true, thresholds=np.arange(0.1, 1.00, 0.01)):\n",
        "    probs = sigmoid(logits)\n",
        "    num_labels = y_true.shape[1]\n",
        "    best_thresholds = np.full(num_labels, 0.5, dtype=float)\n",
        "\n",
        "    for i in range(num_labels):\n",
        "        best_f1, best_t = -1.0, 0.5\n",
        "        yt = y_true[:, i]\n",
        "        pr = probs[:, i]\n",
        "        for t in thresholds:\n",
        "            pred_i = (pr >= t).astype(int)\n",
        "            f1 = f1_score(yt, pred_i, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, t\n",
        "        best_thresholds[i] = best_t\n",
        "\n",
        "    return best_thresholds\n",
        "\n",
        "def eval_with_thresholds(logits, y_true, thresholds):\n",
        "    probs = sigmoid(logits)\n",
        "    y_pred = (probs >= thresholds).astype(int)\n",
        "    # --- Coverage / emptiness metrics ---\n",
        "    pred_counts = y_pred.sum(axis=1)                    # how many labels predicted per sample\n",
        "    coverage = np.mean(pred_counts > 0)                # fraction with at least 1 label\n",
        "    avg_labels = float(np.mean(pred_counts))           # label cardinality (avg predicted labels per sample)\n",
        "    return {\n",
        "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"precision_micro\": precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "        \"recall_micro\": recall_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "                # new: usefulness/behavior metrics\n",
        "        \"coverage_at_least_one\": float(coverage),\n",
        "        \"avg_pred_labels\": avg_labels,\n",
        "    }\n",
        "\n",
        "# Tune thresholds on VAL once\n",
        "val_pred = trainer.predict(val_dataset)\n",
        "best_thresholds = find_best_thresholds(val_pred.predictions, val_pred.label_ids)\n",
        "\n",
        "# Evaluate TEST with frozen thresholds\n",
        "test_pred = trainer.predict(test_dataset)\n",
        "metrics_test = eval_with_thresholds(test_pred.predictions, test_pred.label_ids, best_thresholds)\n",
        "print(metrics_test)\n"
      ],
      "metadata": {
        "id": "3DEQonflhJlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# ===== Where to save =====\n",
        "local_dir = \"./my_trained_model\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# ===== 0) Recover the true label order used in training =====\n",
        "# (Because you created y_train as a DataFrame with columns=mlb.classes_)\n",
        "label_names = list(y_train.columns)\n",
        "\n",
        "# Optional safety checks\n",
        "assert trainer.model.config.num_labels == len(label_names), (\n",
        "    f\"Mismatch: model has {trainer.model.config.num_labels} labels, \"\n",
        "    f\"but y_train has {len(label_names)} columns\"\n",
        ")\n",
        "assert len(best_thresholds) == len(label_names), (\n",
        "    f\"Mismatch: best_thresholds has {len(best_thresholds)} values, \"\n",
        "    f\"but there are {len(label_names)} labels\"\n",
        ")\n",
        "\n",
        "# ===== 1) Write label mapping into config BEFORE saving =====\n",
        "trainer.model.config.id2label = {i: n for i, n in enumerate(label_names)}\n",
        "trainer.model.config.label2id = {n: i for i, n in enumerate(label_names)}\n",
        "trainer.model.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "# ===== 2) Save best model + tokenizer + training args =====\n",
        "trainer.save_model(local_dir)\n",
        "tokenizer.save_pretrained(local_dir)\n",
        "\n",
        "with open(os.path.join(local_dir, \"training_args.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(trainer.args.to_json_string())\n",
        "\n",
        "# ===== 3) Save label names (now guaranteed correct & in the right order) =====\n",
        "with open(os.path.join(local_dir, \"label_names.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(label_names, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ===== 4) Save thresholds =====\n",
        "best_thresholds_list = np.asarray(best_thresholds, dtype=float).tolist()\n",
        "with open(os.path.join(local_dir, \"best_thresholds.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_thresholds_list, f, indent=2)\n",
        "\n",
        "# ===== 5) Save metadata =====\n",
        "metadata = {\n",
        "    \"metric_for_best_model\": trainer.args.metric_for_best_model,\n",
        "    \"best_metric\": trainer.state.best_metric,\n",
        "    \"global_step\": trainer.state.global_step,\n",
        "    \"num_train_epochs\": trainer.state.epoch,\n",
        "}\n",
        "with open(os.path.join(local_dir, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "with open(os.path.join(local_dir, \"log_history.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(trainer.state.log_history, f, indent=2)\n",
        "\n",
        "# ===== 6) Zip + download =====\n",
        "zip_path = shutil.make_archive(\"my_trained_model\", \"zip\", local_dir)\n",
        "print(f\"ðŸ“¦ Zipped to: {zip_path}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_path)\n",
        "except Exception:\n",
        "    print(\"âœ… Not running in Colab â€” zip saved locally.\")\n"
      ],
      "metadata": {
        "id": "FeMsk3TMCCt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# true_avg = test_pred.label_ids.sum(axis=1).mean()\n",
        "# pred_avg = (sigmoid(test_pred.predictions) >= best_thresholds).sum(axis=1).mean()\n",
        "# print(true_avg, pred_avg)\n"
      ],
      "metadata": {
        "id": "-SYO4zoNs73A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv(\"df_test.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"df_test.csv\")\n"
      ],
      "metadata": {
        "id": "7Rb7oDwCoSMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train(resume_from_checkpoint=True)\n",
        "# print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
        "# print(\"Best metric:\", trainer.state.best_metric)\n"
      ],
      "metadata": {
        "id": "ik2lMrHoXf5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}